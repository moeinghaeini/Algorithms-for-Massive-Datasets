{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar Book Reviews using PySpark in Google Colab\n",
    "\n",
    "This notebook implements a system to find pairs of similar book reviews using PySpark. It processes review texts, calculates Jaccard similarity, and identifies pairs above a certain similarity threshold.\n",
    "\n",
    "**Steps:**\n",
    "1. Install Spark and `findspark`.\n",
    "2. Initialize SparkSession.\n",
    "3. Define text preprocessing functions and UDFs.\n",
    "4. Load the dataset (e.g., `Books_rating.csv`). **You will need to upload this file or make it accessible from Google Drive.**\n",
    "5. Preprocess and tokenize review texts.\n",
    "6. Generate candidate pairs of reviews.\n",
    "7. Calculate Jaccard similarity for candidate pairs.\n",
    "8. Filter and display highly similar pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Spark and findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # Important: run this before importing pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, SetType\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Text Preprocessing Functions and Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set([\n",
    "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \n",
    "    \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \n",
    "    \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\",\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n",
    "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \n",
    "    \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \n",
    "    \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \n",
    "    \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \n",
    "    \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \n",
    "    \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \n",
    "    \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
    "])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    words = text.split()\n",
    "    processed_words = set(word for word in words if word not in STOP_WORDS and len(word) > 1)\n",
    "    return list(processed_words)\n",
    "\n",
    "preprocess_text_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "\n",
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1) # Ensure input is a set\n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2) # Ensure input is a set\n",
    "        \n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "        \n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    if union_size == 0:\n",
    "        return 0.0 \n",
    "    return float(intersection_size) / union_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize SparkSession and Load Data\n",
    "\n",
    "**Important:** \n",
    "- Update `dataset_path` to the location of your `Books_rating.csv` file in your Colab environment (e.g., `/content/Books_rating.csv` if uploaded directly, or `/content/drive/MyDrive/path/to/Books_rating.csv` if in Google Drive after mounting).\n",
    "- The dataset is large. For initial runs, using `limit()` is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SimilarBookReviewsColab\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# --- Configuration ---\n",
    "# !!! IMPORTANT: UPDATE THIS PATH !!!\n",
    "dataset_path = \"/content/Books_rating.csv\" # Example path for Colab, replace as needed\n",
    "similarity_threshold = 0.7\n",
    "\n",
    "print(f\"Starting similar book review detection from: {dataset_path}\")\n",
    "print(f\"Similarity threshold: {similarity_threshold}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(dataset_path)\n",
    "    # Handle potential issues with column names containing special characters like '/'\n",
    "    reviews_df = df.select(col(\"Id\").alias(\"review_id\"), col(\"`review/text`\").alias(\"review_text\")).na.drop(subset=[\"review_text\"])\n",
    "    \n",
    "    # FOR DEVELOPMENT: Limit data size. Remove or increase for full run.\n",
    "    reviews_df = reviews_df.limit(1000) \n",
    "    print(f\"Successfully loaded and selected Id and review/text. Initial count: {reviews_df.count()}\")\n",
    "    reviews_df.show(5, truncate=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure 'Books_rating.csv' is uploaded to Colab and the path is correct.\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'reviews_df' in locals(): # Check if DataFrame exists\n",
    "    tokenized_reviews_df = reviews_df.withColumn(\"processed_words\", preprocess_text_udf(col(\"review_text\")))\n",
    "    \n",
    "    processed_rdd = tokenized_reviews_df.select(\"review_id\", \"processed_words\").rdd \\\n",
    "        .map(lambda row: (row.review_id, set(row.processed_words))) \\\n",
    "        .filter(lambda x: len(x[1]) > 0) # Filter out reviews that become empty\n",
    "\n",
    "    processed_rdd.cache()\n",
    "    print(f\"Preprocessing and tokenization complete. RDD count: {processed_rdd.count()}\")\n",
    "    print(\"Sample of processed RDD:\", processed_rdd.take(5))\n",
    "else:\n",
    "    print(\"reviews_df not loaded. Cannot proceed with preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Candidate Pairs (Inverted Index approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'processed_rdd' in locals() and processed_rdd.count() > 0:\n",
    "    # Map 1: (word, review_id)\n",
    "    word_to_review_id_rdd = processed_rdd.flatMap(lambda x: [(word, x[0]) for word in x[1]])\n",
    "    # print(\"Sample of word_to_review_id_rdd:\", word_to_review_id_rdd.take(10))\n",
    "\n",
    "    # Reduce 1 (Group by word): (word, [review_id1, review_id2, ...])\n",
    "    word_to_review_ids_list_rdd = word_to_review_id_rdd.groupByKey().mapValues(list)\n",
    "    # print(\"Sample of word_to_review_ids_list_rdd:\", word_to_review_ids_list_rdd.take(5))\n",
    "\n",
    "    # Map 2 (Generate pairs)\n",
    "    candidate_pairs_rdd = word_to_review_ids_list_rdd.flatMap(\n",
    "        lambda x: [tuple(sorted(pair)) for pair in combinations(x[1], 2)]\n",
    "    ).distinct()\n",
    "    \n",
    "    candidate_pairs_rdd.cache()\n",
    "    print(f\"Candidate pair generation complete. Number of candidate pairs: {candidate_pairs_rdd.count()}\")\n",
    "    # print(\"Sample of candidate_pairs_rdd:\", candidate_pairs_rdd.take(10))\n",
    "else:\n",
    "    print(\"processed_rdd not available or empty. Cannot generate candidate pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Jaccard Similarity for Candidate Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'candidate_pairs_rdd' in locals() and 'processed_rdd' in locals() and candidate_pairs_rdd.count() > 0:\n",
    "    review_word_sets_rdd = processed_rdd # (review_id, word_set)\n",
    "\n",
    "    # Join candidate pairs with word sets\n",
    "    # pair: (id1, id2)\n",
    "    # Need to join twice to get word sets for both ids in the pair\n",
    "    pairs_with_set1 = candidate_pairs_rdd.map(lambda pair: (pair[0], pair[1])) \\\n",
    "        .join(review_word_sets_rdd) \\\n",
    "        .map(lambda x: (x[1][0], (x[0], x[1][1]))) # (id2, (id1, word_set1))\n",
    "    \n",
    "    joined_pairs_with_sets = pairs_with_set1.join(review_word_sets_rdd) \\\n",
    "        .map(lambda x: ((x[1][0][0], x[0]), (x[1][0][1], x[1][1]))) # ((id1, id2), (set1, set2))\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    # Need to ensure the calculate_jaccard_similarity function is available to executors\n",
    "    # It's defined globally in a cell above, so it should be fine.\n",
    "    similarities_rdd = joined_pairs_with_sets.map(\n",
    "        lambda x: (x[0], calculate_jaccard_similarity(x[1][0], x[1][1]))\n",
    "    )\n",
    "    # print(\"Sample of similarities_rdd:\", similarities_rdd.take(10))\n",
    "else:\n",
    "    print(\"Candidate pairs or processed_rdd not available/empty. Cannot calculate similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Filter and Output Similar Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'similarities_rdd' in locals():\n",
    "    highly_similar_pairs_rdd = similarities_rdd.filter(lambda x: x[1] >= similarity_threshold)\n",
    "    \n",
    "    print(f\"Found {highly_similar_pairs_rdd.count()} pairs with similarity >= {similarity_threshold}\")\n",
    "\n",
    "    results = highly_similar_pairs_rdd.collect()\n",
    "    if results:\n",
    "        print(f\"\\n--- Highly Similar Review Pairs (Similarity >= {similarity_threshold}) ---\")\n",
    "        for pair, similarity in results:\n",
    "            print(f\"Review Pair: {pair[0]} - {pair[1]}, Similarity: {similarity:.4f}\")\n",
    "    else:\n",
    "        print(\"No pairs found above the similarity threshold.\")\n",
    "else:\n",
    "    print(\"similarities_rdd not available. Cannot filter results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
