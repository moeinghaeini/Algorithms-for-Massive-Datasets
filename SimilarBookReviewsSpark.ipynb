{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Similar Book Reviews using PySpark, MinHashing, and LSH\n",
    "\n",
    "This notebook implements a system to find pairs of similar book reviews from the Amazon Book Reviews dataset. It leverages PySpark for distributed processing and employs MinHashing with Locality Sensitive Hashing (LSH) to efficiently identify candidate pairs for Jaccard similarity calculation.\n",
    "\n",
    "**Core Pipeline Stages:**\n",
    "1.  **Setup:** Install and initialize Spark and necessary NLP libraries (NLTK).\n",
    "2.  **Data Loading:** Load the book reviews dataset.\n",
    "3.  **Advanced Text Preprocessing:** Clean and transform review text using techniques like lowercasing, punctuation removal, tokenization, POS tagging, and lemmatization. Stop words are also removed.\n",
    "4.  **Shingling:** Convert preprocessed review texts into sets of character k-shingles.\n",
    "5.  **MinHashing:** Generate MinHash signatures for each review's shingle set to create compact representations that preserve Jaccard similarity.\n",
    "6.  **Locality Sensitive Hashing (LSH):** Use the banding technique on MinHash signatures to identify candidate pairs of reviews that are likely to be similar, significantly reducing the number of direct comparisons needed.\n",
    "7.  **Jaccard Similarity Calculation:** Compute the exact Jaccard similarity for the candidate pairs identified by LSH using their shingle sets.\n",
    "8.  **Filtering & Results:** Filter pairs based on a similarity threshold and display the highly similar ones.\n",
    "9.  **Initial Data Visualizations:** Provide some basic plots to understand data characteristics and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Spark, NLTK, and Helper Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "!pip install -q pyspark\n",
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK Resources (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Environment & Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # Important: run this before importing pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from itertools import combinations\n",
    "\n",
    "# NLTK imports for advanced preprocessing\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Visualization libraries (ensure they are installed in the first cell if not already)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Text Preprocessing Logic\n",
    "\n",
    "This involves:\n",
    "- Lowercasing text.\n",
    "- Removing punctuation.\n",
    "- Tokenizing text into words using NLTK.\n",
    "- Performing Part-of-Speech (POS) tagging.\n",
    "- Lemmatizing words to their base form using their POS tags for better accuracy.\n",
    "- Removing numbers and short tokens.\n",
    "- Filtering out common English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text_enhanced(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tagged_tokens:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma.isdigit() and len(lemma) > 1:\n",
    "            lemmatized_tokens.append(lemma)\n",
    "    processed_words = set(word for word in lemmatized_tokens if word not in STOP_WORDS)\n",
    "    return list(processed_words)\n",
    "\n",
    "preprocess_text_udf = udf(preprocess_text_enhanced, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Jaccard Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1)\n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2)\n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    if union_size == 0:\n",
    "        return 0.0 \n",
    "    return float(intersection_size) / union_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters\n",
    "\n",
    "Set the main parameters for data loading, LSH, and similarity calculations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset and General Parameters ---\n",
    "# !!! IMPORTANT: UPDATE THIS PATH if not using the default Colab upload path !!!\n",
    "DATASET_PATH = \"/content/Books_rating.csv\" \n",
    "SIMILARITY_THRESHOLD = 0.7 # Jaccard similarity threshold to consider pairs as similar\n",
    "DATA_LIMIT = 1000 # For development: Limit data size. Set to None or remove for full run.\n",
    "\n",
    "# --- MinHashing and LSH Parameters ---\n",
    "K_SHINGLE = 9  # Length of k-shingles (e.g., 5 or 9 characters)\n",
    "NUM_HASH_FUNCTIONS = 100  # Number of MinHash functions (e.g., 100 or 200)\n",
    "LSH_BANDS = 20  # Number of bands for LSH\n",
    "# LSH_ROWS is derived: NUM_HASH_FUNCTIONS // LSH_BANDS\n",
    "\n",
    "MAX_HASH_VAL = (1 << 32) - 1 # Max value for hash signatures in MinHash\n",
    "HASH_SEEDS = list(range(NUM_HASH_FUNCTIONS)) # Seeds for MinHash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize SparkSession and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "# Add any custom stop words if needed\n",
    "# STOP_WORDS.update([\"book\", \"read\", \"review\"])\n",
    "\n",
    "# Helper function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text_enhanced(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. POS Tagging\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # 5. Lemmatization & Number Removal\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tagged_tokens:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma.isdigit() and len(lemma) > 1: # Remove numbers and short tokens here\n",
    "            lemmatized_tokens.append(lemma)\n",
    "            \n",
    "    # 6. Remove stop words\n",
    "    processed_words = set(word for word in lemmatized_tokens if word not in STOP_WORDS)\n",
    "    \n",
    "    return list(processed_words)\n",
    "\n",
    "preprocess_text_udf = udf(preprocess_text_enhanced, ArrayType(StringType()))\n",
    "\n",
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1) # Ensure input is a set\n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2) # Ensure input is a set\n",
    "        \n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "        \n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    if union_size == 0:\n",
    "        return 0.0 \n",
    "    return float(intersection_size) / union_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize SparkSession and Load Data\n",
    "\n",
    "**Important:** \n",
    "- Update `dataset_path` to the location of your `Books_rating.csv` file in your Colab environment (e.g., `/content/Books_rating.csv` if uploaded directly, or `/content/drive/MyDrive/path/to/Books_rating.csv` if in Google Drive after mounting).\n",
    "- The dataset is large. For initial runs, using `limit()` is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SimilarBookReviewsColab\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# --- Configuration ---\n",
    "# !!! IMPORTANT: UPDATE THIS PATH !!!\n",
    "dataset_path = \"/content/Books_rating.csv\" # Example path for Colab, replace as needed\n",
    "similarity_threshold = 0.7\n",
    "\n",
    "print(f\"Starting similar book review detection from: {dataset_path}\")\n",
    "print(f\"Similarity threshold: {similarity_threshold}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(dataset_path)\n",
    "    # Handle potential issues with column names containing special characters like '/'\n",
    "    reviews_df = df.select(col(\"Id\").alias(\"review_id\"), col(\"`review/text`\").alias(\"review_text\")).na.drop(subset=[\"review_text\"])\n",
    "    \n",
    "    # FOR DEVELOPMENT: Limit data size. Remove or increase for full run.\n",
    "    reviews_df = reviews_df.limit(1000) \n",
    "    print(f\"Successfully loaded and selected Id and review/text. Initial count: {reviews_df.count()}\")\n",
    "    reviews_df.show(5, truncate=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure 'Books_rating.csv' is uploaded to Colab and the path is correct.\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'reviews_df' in locals(): # Check if DataFrame exists\n",
    "    tokenized_reviews_df = reviews_df.withColumn(\"processed_words\", preprocess_text_udf(col(\"review_text\")))\n",
    "    \n",
    "    processed_rdd = tokenized_reviews_df.select(\"review_id\", \"processed_words\").rdd \\\n",
    "        .map(lambda row: (row.review_id, set(row.processed_words))) \\\n",
    "        .filter(lambda x: len(x[1]) > 0) # Filter out reviews that become empty\n",
    "\n",
    "    processed_rdd.cache()\n",
    "    print(f\"Preprocessing and tokenization complete. RDD count: {processed_rdd.count()}\")\n",
    "    print(\"Sample of processed RDD:\", processed_rdd.take(5))\n",
    "else:\n",
    "    print(\"reviews_df not loaded. Cannot proceed with preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Candidate Pairs (Inverted Index approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'processed_rdd' in locals() and processed_rdd.count() > 0:\n",
    "    # This cell implements Shingling, MinHashing, and LSH for candidate pair generation.\n",
    "    # It replaces the previous inverted index approach for candidate pairs.\n",
    "\n",
    "    # Parameters for MinHashing and LSH\n",
    "    K_SHINGLE = 9  # Length of k-shingles (e.g., 5 or 9 characters)\n",
    "    NUM_HASH_FUNCTIONS = 100  # Number of MinHash functions (e.g., 100 or 200)\n",
    "    LSH_BANDS = 20  # Number of bands for LSH (e.g., 20 if NUM_HASH_FUNCTIONS=100, giving 5 rows per band)\n",
    "    LSH_ROWS = NUM_HASH_FUNCTIONS // LSH_BANDS # Rows per band\n",
    "\n",
    "    if LSH_ROWS == 0:\n",
    "        raise ValueError(\"NUM_HASH_FUNCTIONS must be divisible by LSH_BANDS and result in at least 1 row per band.\")\n",
    "\n",
    "    print(f\"\\n--- Starting Shingling, MinHashing & LSH ---\")\n",
    "    print(f\"K-shingle length: {K_SHINGLE}\")\n",
    "    print(f\"Number of hash functions for MinHash: {NUM_HASH_FUNCTIONS}\")\n",
    "    print(f\"LSH Bands: {LSH_BANDS}, Rows per band: {LSH_ROWS}\")\n",
    "\n",
    "    # --- c1. Shingling ---\n",
    "    def create_shingles(lemmas_list, k):\n",
    "        if not lemmas_list:\n",
    "            return set()\n",
    "        # Join lemmas into a single string to create character shingles\n",
    "        doc_str = \"\".join(sorted(list(lemmas_list))).replace(\" \", \"\") # sorted list of lemmas joined\n",
    "        if len(doc_str) < k:\n",
    "            return {doc_str} if doc_str else set()\n",
    "        shingles = set()\n",
    "        for i in range(len(doc_str) - k + 1):\n",
    "            shingles.add(doc_str[i:i+k])\n",
    "        return list(shingles) # Return list for Spark UDF\n",
    "\n",
    "    # UDF for shingling. Input is 'processed_words' (list of lemmas from previous step)\n",
    "    create_shingles_udf = udf(lambda lemmas: create_shingles(lemmas, K_SHINGLE), ArrayType(StringType()))\n",
    "\n",
    "    # Apply shingling to the 'processed_words' (which are lemmas)\n",
    "    # tokenized_reviews_df is from cell 5 (Preprocessing & Tokenization)\n",
    "    shingled_reviews_df = tokenized_reviews_df.withColumn(\"shingles\", create_shingles_udf(col(\"processed_words\")))\n",
    "    \n",
    "    # RDD of (review_id, {shingle_set})\n",
    "    shingled_rdd = shingled_reviews_df.select(\"review_id\", \"shingles\").rdd \\\n",
    "        .map(lambda row: (row.review_id, set(row.shingles))) \\\n",
    "        .filter(lambda x: len(x[1]) > 0)\n",
    "    \n",
    "    shingled_rdd.cache()\n",
    "    print(f\"Shingling complete. RDD count: {shingled_rdd.count()}\")\n",
    "    # print(\"Sample of shingled RDD:\", shingled_rdd.take(2))\n",
    "\n",
    "    # --- c2. MinHash Signature Generation ---\n",
    "    MAX_HASH_VAL = (1 << 32) - 1 # Max value for hash signatures\n",
    "    hash_seeds = list(range(NUM_HASH_FUNCTIONS)) # Seeds for our hash functions\n",
    "\n",
    "    def generate_minhash_signature(shingle_set, num_hashes, seeds_list):\n",
    "        signature = []\n",
    "        if not shingle_set:\n",
    "            return [MAX_HASH_VAL] * num_hashes\n",
    "\n",
    "        for i in range(num_hashes):\n",
    "            min_hash_for_this_func = MAX_HASH_VAL\n",
    "            for shingle in shingle_set:\n",
    "                shingle_str = str(shingle) # Ensure shingle is a string\n",
    "                # Simple hash: (hash(shingle + seed)) % MAX_HASH_VAL. Python's hash() is okay for single run.\n",
    "                # For production, use a more robust hash like mmh3.hash with the seed.\n",
    "                current_hash = abs(hash(shingle_str + str(seeds_list[i]))) % MAX_HASH_VAL \n",
    "                if current_hash < min_hash_for_this_func:\n",
    "                    min_hash_for_this_func = current_hash\n",
    "            signature.append(min_hash_for_this_func)\n",
    "        return signature\n",
    "\n",
    "    minhash_signatures_rdd = shingled_rdd.map(\n",
    "        lambda x: (x[0], generate_minhash_signature(x[1], NUM_HASH_FUNCTIONS, hash_seeds))\n",
    "    )\n",
    "    minhash_signatures_rdd.cache()\n",
    "    print(f\"MinHash signature generation complete. RDD count: {minhash_signatures_rdd.count()}\")\n",
    "    # print(\"Sample MinHash Signatures:\", minhash_signatures_rdd.take(2))\n",
    "\n",
    "    # --- c3. LSH Banding for Candidate Pairs ---\n",
    "    def generate_lsh_bands(signature_item, bands, rows):\n",
    "        review_id, signature_vector = signature_item\n",
    "        band_kv_pairs = []\n",
    "        if not signature_vector or len(signature_vector) != bands * rows:\n",
    "            # print(f\"Warning: Signature for {review_id} has unexpected length or is empty.\")\n",
    "            return band_kv_pairs \n",
    "\n",
    "        for i in range(bands):\n",
    "            start_index = i * rows\n",
    "            end_index = start_index + rows\n",
    "            band_content = tuple(signature_vector[start_index:end_index])\n",
    "            bucket_key = (i, hash(band_content)) # (band_id, hash_of_band_content)\n",
    "            band_kv_pairs.append((bucket_key, review_id))\n",
    "        return band_kv_pairs\n",
    "\n",
    "    banded_rdd = minhash_signatures_rdd.flatMap(\n",
    "        lambda sig_item: generate_lsh_bands(sig_item, LSH_BANDS, LSH_ROWS)\n",
    "    )\n",
    "    \n",
    "    bucketed_reviews_rdd = banded_rdd.groupByKey().mapValues(list)\n",
    "\n",
    "    candidate_pairs_lsh_rdd = bucketed_reviews_rdd.flatMap(\n",
    "        lambda x: [tuple(sorted(pair)) for pair in combinations(x[1], 2) if len(x[1]) > 1]\n",
    "    ).distinct()\n",
    "\n",
    "    candidate_pairs_lsh_rdd.cache()\n",
    "    num_candidate_pairs = candidate_pairs_lsh_rdd.count()\n",
    "    print(f\"LSH complete. Number of candidate pairs: {num_candidate_pairs}\")\n",
    "    # print(\"Sample LSH candidate pairs:\", candidate_pairs_lsh_rdd.take(10))\n",
    "else:\n",
    "    print(\"processed_rdd not available or empty. Cannot generate candidate pairs with LSH.\")\n",
    "    candidate_pairs_lsh_rdd = sc.emptyRDD() # Ensure it exists even if empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Jaccard Similarity for LSH Candidate Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure candidate_pairs_lsh_rdd exists from the previous LSH step\n",
    "# Ensure shingled_rdd exists, containing (review_id, {shingle_set})\n",
    "if 'candidate_pairs_lsh_rdd' in locals() and 'shingled_rdd' in locals() and candidate_pairs_lsh_rdd.count() > 0:\n",
    "    # We need the *original shingle sets* for the candidate pairs identified by LSH.\n",
    "    # shingled_rdd contains (review_id, {shingle_set})\n",
    "\n",
    "    # Join candidate pairs with shingle sets\n",
    "    # pair: (id1, id2)\n",
    "    pairs_with_set1 = candidate_pairs_lsh_rdd.map(lambda pair: (pair[0], pair[1])) \\\n",
    "        .join(shingled_rdd) \\\n",
    "        .map(lambda x: (x[1][0], (x[0], x[1][1]))) # (id2, (id1, shingle_set1))\n",
    "    \n",
    "    joined_pairs_with_sets = pairs_with_set1.join(shingled_rdd) \\\n",
    "        .map(lambda x: ((x[1][0][0], x[0]), (x[1][0][1], x[1][1]))) # ((id1, id2), (shingle_set1, shingle_set2))\n",
    "\n",
    "    # Calculate Jaccard similarity using the shingle sets\n",
    "    similarities_rdd = joined_pairs_with_sets.map(\n",
    "        lambda x: (x[0], calculate_jaccard_similarity(x[1][0], x[1][1]))\n",
    "    )\n",
    "    # print(\"Sample similarities_rdd (LSH candidates):\", similarities_rdd.take(10))\n",
    "else:\n",
    "    print(\"LSH candidate pairs or shingled_rdd not available/empty. Cannot calculate similarity.\")\n",
    "    similarities_rdd = sc.emptyRDD() # Ensure it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Filter and Output Similar Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'similarities_rdd' in locals():\n",
    "    highly_similar_pairs_rdd = similarities_rdd.filter(lambda x: x[1] >= similarity_threshold)\n",
    "    \n",
    "    print(f\"Found {highly_similar_pairs_rdd.count()} pairs with similarity >= {similarity_threshold}\")\n",
    "\n",
    "    results = highly_similar_pairs_rdd.collect()\n",
    "    if results:\n",
    "        print(f\"\\n--- Highly Similar Review Pairs (Similarity >= {similarity_threshold}) ---\")\n",
    "        for pair, similarity in results:\n",
    "            print(f\"Review Pair: {pair[0]} - {pair[1]}, Similarity: {similarity:.4f}\")\n",
    "    else:\n",
    "        print(\"No pairs found above the similarity threshold.\")\n",
    "else:\n",
    "    print(\"similarities_rdd not available. Cannot filter results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initial Data Visualizations\n",
    "\n",
    "This section provides some basic visualizations of the data and results. For large datasets, collecting full data for plotting might be slow or lead to memory issues; sampling or using Spark's built-in summary statistics would be more appropriate. Given the `.limit(1000)` for development, direct collection is feasible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ensure pyspark is installed if running locally and not in Colab's default env\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError:\n",
    "    print(\"Attempting to install pyspark for plotting context if needed...\")\n",
    "    !pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Review Length Distribution (Number of Lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'processed_rdd' in locals() and processed_rdd.count() > 0:\n",
    "    review_lengths = processed_rdd.map(lambda x: len(x[1])).collect()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(review_lengths, bins=50, kde=False)\n",
    "    plt.title('Distribution of Review Lengths (Number of Lemmas)')\n",
    "    plt.xlabel('Number of Lemmas')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Vocabulary Size\n",
    "    vocabulary = processed_rdd.flatMap(lambda x: list(x[1])).distinct()\n",
    "    vocab_size = vocabulary.count()\n",
    "    print(f\"Total unique lemmas (Vocabulary Size): {vocab_size}\")\n",
    "else:\n",
    "    print(\"processed_rdd not available or empty. Cannot plot review length distribution or vocab size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Shingle Set Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shingled_rdd' in locals() and shingled_rdd.count() > 0:\n",
    "    shingle_set_sizes = shingled_rdd.map(lambda x: len(x[1])).collect()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(shingle_set_sizes, bins=50, kde=False)\n",
    "    plt.title('Distribution of Shingle Set Sizes per Document')\n",
    "    plt.xlabel('Number of Shingles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"shingled_rdd not available or empty. Cannot plot shingle set size distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Number of Candidate Pairs from LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'candidate_pairs_lsh_rdd' in locals():\n",
    "    num_lsh_candidates = candidate_pairs_lsh_rdd.count()\n",
    "    print(f\"Number of candidate pairs generated by LSH: {num_lsh_candidates}\")\n",
    "    \n",
    "    # For comparison, if you had the old candidate generation method's count:\n",
    "    # old_candidate_count = ... \n",
    "    # print(f\"Number of candidate pairs (old method): {old_candidate_count}\")\n",
    "    # print(f\"Reduction in candidate pairs by LSH: { (1 - (num_lsh_candidates / old_candidate_count)) * 100 if old_candidate_count > 0 else 'N/A'}%\")\n",
    "else:\n",
    "    print(\"candidate_pairs_lsh_rdd not available. Cannot show number of LSH candidates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Distribution of Jaccard Similarities for LSH Candidate Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'similarities_rdd' in locals() and similarities_rdd.count() > 0:\n",
    "    jaccard_scores = similarities_rdd.map(lambda x: x[1]).collect()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(jaccard_scores, bins=30, kde=True)\n",
    "    plt.title('Distribution of Jaccard Similarities for LSH Candidate Pairs')\n",
    "    plt.xlabel('Jaccard Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    highly_similar_count = 0\n",
    "    if 'highly_similar_pairs_rdd' in locals():\n",
    "        highly_similar_count = highly_similar_pairs_rdd.count()\n",
    "    print(f\"Number of pairs with Jaccard similarity >= {similarity_threshold}: {highly_similar_count}\")\n",
    "else:\n",
    "    print(\"similarities_rdd not available or empty. Cannot plot Jaccard similarity distribution.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.x"  # Updated to a generic x version
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
